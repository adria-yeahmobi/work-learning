# 生成模型基本原理：
给定两组数据x和z，z服从已知的简单先验分布（通常是高斯分布），x服从复杂的分布（x是训练数据的分布），生成模型是寻找一个函数，让每一个z采样点都可以找到对应的样本点x（新的或者原本的训练数据）
## 其他生成模型：
### GAN(Generative Adversarial Networks)
模型原理：
一个生成器和一个判别器组成，生成器生成的数据由判别器判断真假，两个模型相互学习
### VAE(Variational Auto Encoder)变分自编码器：
核心思想：
原本空白的编码点会生成失真图像（编码解码过程是非线性的，点与点空间的变换没有规律），于是在编码器编码过程加入噪声，扩大编码点的范围，涵盖了失真编码点，可以生成与两侧已知编码点相似的图片，但是仍然是离散的点，拉长噪音（仍保证原编码点编码概率最高），图像编码点的分布成为连续的。
模型原理：
将采样到的分布概率映射到训练集的分布概率，生成隐变量，包含数据信息和噪声信息，可以还原输入数据，还可以生成新的数据
## 流模型（Flow-based Model）
基本原理：
计算分布转换的积分式，得到输入数据和生成数据的双工通道（基本原理数学计算很复杂）
# 理论部分：
## 马尔可夫链：
- 相关：
马尔可夫性：在状态空间中，由一个状态转换到另外一个状态的随机过程，要求有“无记忆性”，即下一个状态的概率分布只与当前状态有关，在时间序列中前面的事件均无关
马尔科夫链认为：过去所有的信息都被保存在当前状态中了
- 数学定义：
![图片](https://github.com/adria-yeahmobi/work-learning/blob/main/images/Markoff_chain.png)
- 状态转移概率矩阵：
作用：直观地描述所有状态转换的概率
方便计算：基本的状态转换概率矩阵P，求n次运动后状态转换的概率只要求P的n次幂对应位置的概率就可
特性：在经过一定有限次数的序列变换，可以得到一个稳定的概率分布，与初始概率无关（连续状态也成立）
## 机器学习中的应用：
语言模型：基于独立输入假设（第n个词的出现只与第n-1个词有关
声学模型：语音识别系统
# 扩散模型工作原理：
连续添加噪声破坏训练数据，通过反转噪声过程，学习恢复数据
- 训练目标：学习正向添加噪声的反过程
- 组成：扩散过程和逆扩散过程
- 数学推导：https://blog.csdn.net/DFCED/article/details/132394895       （有点复杂）
