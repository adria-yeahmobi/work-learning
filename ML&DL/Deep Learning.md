# 灾难性遗忘
灾难性遗忘/干扰(catcastrophic forgetting/interference)：在连续学习多个任务的过程中，，学习新知识的过程会遗忘旧知识，从而导致模型性能在旧任务中急剧下降
## 原因：
传统模型假设数据分布是固定或平稳的，训练样本iid分布，模型训练过程见到的数据可以反复学习，但是当数据变为数据流时，训练数据的分布是非平稳的，新知识会干扰旧知识，甚至覆盖
## 缓解方法
- 冻结重要权重
- 持续性学习，可以缓解灾难性遗忘的机器学习方法，包括正则化方法/记忆回放reply-based/参数孤立等
-- 记忆回放：基于样本：训练新任务的时候混一些旧任务的数据样本；基于特征：存储量化后的特征图，训练新任务的时候从记忆库重构出记忆和新数据一起训练
# 对抗生成网络GAN的loss理解
![loss公式](https://github.com/adria-yeahmobi/work-learning/blob/main/images/GAN-loss.png)
GAN结构：生成器G 对抗器D  
公式拆成左右两部分来看  
[简单理解参考](https://blog.csdn.net/qq_40714949/article/details/120493934)

# 神经网络
全连接神经网络是一种基本的神经网络模型，具有参数最多和计算量大的特点。一般由输入层、隐藏层和输出层组成。  
输入层和输出层仅各有一层，而隐藏层可由多个网络层组成。  
每个网络层中都含有许多神经元，相邻的网络层之间的神经元相互连接，而同层的神经元互不相连。  
全连接神经网络根据隐藏层的数量，又可分为浅层神经网络和深度神经网络。其中，深度神经网络的隐藏层数量至少大于2。  
深度神经网络相较于浅层神经网络具有更强的表达能力，且能够拟合成所有映射函数，但是深度神经网络的参数数量一般较多，不易训练，需要大量的数据集。
